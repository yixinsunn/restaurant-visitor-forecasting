{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('C:/Users/yixin/Desktop/Machine_Learning_Projects/restaurant-visitor-forecasting')\n",
    "\n",
    "import helper\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import great_circle\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn import metrics, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define variables\n",
    "seed = 2018\n",
    "path = 'C:/Users/yixin/Desktop/Machine_Learning_Projects/restaurant-visitor-forecasting'\n",
    "# features that will be one-hot encoded\n",
    "cat = ['day_of_week', 'weekend_holiday', 'air_genre_name', 'prefecture']\n",
    "# features that will be target encoded\n",
    "agg = ['day_of_week', 'year', 'weekend_holiday',\n",
    "       'air_store_id']\n",
    "# Some engineered features are unhelpful, and will be dropped\n",
    "drop = ['visit_date', 'visitors', 'air_area_name', 'month', 'weight',\n",
    "        'count_reserve_1', 'count_reserve_2', 'count_reserve_3', 'count_reserve_7', \n",
    "        'count_reserve_14', 'count_reserve_21', 'count_reserve_28', 'count_reserve_35']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = {\n",
    "    'air_reserve': pd.read_csv(path + '/input/air_reserve.csv', \\\n",
    "                               parse_dates=['visit_datetime', 'reserve_datetime']),\n",
    "    'hpg_reserve': pd.read_csv(path + '/input/hpg_reserve.csv', \\\n",
    "                               parse_dates=['visit_datetime', 'reserve_datetime']),\n",
    "    'air_visit': pd.read_csv(path + '/input/air_visit_data.csv', parse_dates=['visit_date']), # training set\n",
    "    'holidays': pd.read_csv(path + '/input/date_info.csv', parse_dates=['calendar_date']).rename(\n",
    "        columns={'calendar_date': 'visit_date'}),\n",
    "    'air_store': pd.read_csv(path + '/input/air_store_info.csv'),\n",
    "    'id': pd.read_csv(path + '/input/store_id_relation.csv'),\n",
    "    'submission': pd.read_csv(path + '/input/sample_submission.csv'),  # test set\n",
    "}\n",
    "train_window = pd.read_csv(path + '/output/train_window.csv', parse_dates=['visit_date'])\n",
    "test_window = pd.read_csv(path + '/output/test_window.csv', parse_dates=['visit_date'])\n",
    "\n",
    "data['hpg_reserve'] = pd.merge(data['hpg_reserve'], data['id'],\n",
    "                               how='inner', on='hpg_store_id').drop('hpg_store_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "###                                      Feature Engineering                                        ###\n",
    "#######################################################################################################\n",
    "# Add day of week, month, year, etc into air visit dataset\n",
    "data['submission']['visit_date'] = data['submission']['id'].apply(lambda x:x[-10:])\n",
    "data['submission']['visit_date'] = pd.to_datetime(data['submission']['visit_date'])\n",
    "data['submission']['air_store_id'] = data['submission']['id'].apply(lambda x:x[:-11])\n",
    "data['submission'].drop('id', axis=1, inplace=True)\n",
    "\n",
    "for df in ['air_visit', 'submission']:\n",
    "    data[df]['day_of_week'] = data[df]['visit_date'].dt.dayofweek\n",
    "    data[df]['month'] = data[df]['visit_date'].dt.month\n",
    "    data[df]['year'] = data[df]['visit_date'].dt.year\n",
    "\n",
    "le = LabelEncoder()\n",
    "train, test = data['air_visit'], data['submission']\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)   # possible\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)     # possible\n",
    "train['year'] = le.fit_transform(train['year'])\n",
    "test['year'] = le.transform(test['year'])\n",
    "\n",
    "visit_date = pd.DataFrame(data['air_visit']['visit_date'].unique()).rename(columns={0: 'visit_date'})\n",
    "visit_date = visit_date.sort_values('visit_date').reset_index(drop=True)\n",
    "visit_date['weight'] = ((visit_date.index + 1) / visit_date.shape[0]) ** 5\n",
    "train = train.merge(visit_date, on='visit_date', how='left')\n",
    "test = test.merge(visit_date, on='visit_date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregate reserve_visitors grouped by each store and visit date\n",
    "reserve = pd.concat([data['air_reserve'], data['hpg_reserve']])\n",
    "reserve['visit_date'] = reserve['visit_datetime'].dt.date\n",
    "reserve['visit_date'] = pd.to_datetime(reserve['visit_date'])\n",
    "\n",
    "tmp1 = reserve.groupby(['air_store_id', 'visit_date'], as_index=False)[\n",
    "    'reserve_visitors'].sum().rename(columns={'reserve_visitors': 'sum_reserve'})\n",
    "tmp2 = reserve.groupby(['air_store_id', 'visit_date'], as_index=False)[\n",
    "    'reserve_visitors'].mean().rename(columns={'reserve_visitors': 'mean_reserve'})\n",
    "reserve = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id', 'visit_date'])\n",
    "\n",
    "# Merge training and test sets with reserve\n",
    "train = train.merge(reserve, how='left', on=['air_store_id', 'visit_date'])\n",
    "test = test.merge(reserve, how='left', on=['air_store_id', 'visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Generate rolling time windows data\n",
    "##train_window = helper.rolling_window(train, reserve, columns=['sum_reserve', 'count_reserve', 'mean_reserve'])\n",
    "##test_window = helper.rolling_window(test, reserve, columns=['sum_reserve', 'count_reserve', 'mean_reserve'])\n",
    "\n",
    "# Merge training and test sets with windows\n",
    "train = train.merge(train_window, how='left', on=['air_store_id', 'visit_date'])\n",
    "test = test.merge(test_window, how='left', on=['air_store_id', 'visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add weekend_holiday categories into holidays data\n",
    "weekday = ['Monday', 'Tuesday', 'Wednesday', 'Thursday']\n",
    "weekend = ['Friday', 'Saturday', 'Sunday']\n",
    "data['holidays']['weekend_holiday'] = 'Nonweekend/Nonholiday'\n",
    "data['holidays']['weekend_holiday'][(data['holidays']['day_of_week'].isin(weekday)) &\n",
    "                                    (data['holidays']['holiday_flg'] == 1)] = 'Nonweekend/Holiday'\n",
    "data['holidays']['weekend_holiday'][(data['holidays']['day_of_week'].isin(weekend)) & \n",
    "                                    (data['holidays']['holiday_flg'] == 0)] = 'Weekend/Nonholiday'\n",
    "data['holidays']['weekend_holiday'][(data['holidays']['day_of_week'].isin(weekend)) & \n",
    "                                    (data['holidays']['holiday_flg'] == 1)] = 'Weekend/Holiday'\n",
    "data['holidays'].drop(['day_of_week', 'holiday_flg'], axis=1, inplace=True)\n",
    "\n",
    "# Merge training and test sets with holidays\n",
    "train = train.merge(data['holidays'], how='left', on='visit_date')\n",
    "test = test.merge(data['holidays'], how='left', on='visit_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add engineered features into air_store\n",
    "# distance difference (reference is median latitude and median longitude)\n",
    "ref = data['air_store'][['latitude', 'longitude']].median().values\n",
    "data['air_store']['diff_dist'] = data['air_store'].apply(lambda x: \\\n",
    "                                great_circle((x['latitude'],x['longitude']), ref).km, axis = 1)\n",
    "# location difference (reference is median latitude and median longitude)\n",
    "data['air_store']['diff_lat'] = np.absolute(\n",
    "    data['air_store']['latitude'].median() - data['air_store']['latitude'])\n",
    "data['air_store']['diff_long'] = np.absolute(\n",
    "    data['air_store']['longitude'].median() - data['air_store']['longitude'])\n",
    "# prefecture\n",
    "data['air_store']['prefecture'] = data['air_store']['air_area_name'].apply(lambda x:str(x).split(' ')[0])\n",
    "# number of restaurants per area\n",
    "tmp = data['air_store'].groupby('air_area_name', as_index=False)['air_store_id'].count().rename(\n",
    "    columns={'air_store_id': 'rest_per_area'})\n",
    "data['air_store'] = pd.merge(data['air_store'], tmp, how='left', on='air_area_name')\n",
    "# number of genre per area\n",
    "tmp = data['air_store'].groupby('air_area_name')['air_genre_name'].nunique().reset_index().rename(\n",
    "    columns={'air_genre_name': 'genre_per_area'})\n",
    "data['air_store'] = pd.merge(data['air_store'], tmp, how='left', on='air_area_name')\n",
    "# number of restaurants per area per genre\n",
    "tmp = data['air_store'].groupby(['air_area_name', 'air_genre_name'], as_index=False)[\n",
    "    'air_store_id'].count().rename(columns={'air_store_id': 'rest_per_area_grouped_by_genre'})\n",
    "data['air_store'] = pd.merge(data['air_store'], tmp, how='left', on=['air_area_name', 'air_genre_name'])\n",
    "# air_store_id\n",
    "tmp = data['air_store'].sort_values(['longitude', 'latitude'])\n",
    "strId_to_intId = dict(zip(tmp['air_store_id'], [i for i in range(tmp.shape[0])]))\n",
    "data['air_store']['id'] = data['air_store']['air_store_id'].apply(lambda x:strId_to_intId[x])\n",
    "\n",
    "# Merge training and test sets with air_store\n",
    "train = pd.merge(train, data['air_store'], how='left', on='air_store_id')\n",
    "test = pd.merge(test, data['air_store'], how='left', on='air_store_id')\n",
    "train = train[train['air_store_id'].isin(test['air_store_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on train: 0.9978859199572827, AUC on test: 0.9974262842099478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((11330, 48), 817)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adversarial validation \n",
    "train, test = helper.get_isTest(train, test, cat, \n",
    "                                [c for c in drop if c!='month' and c!='isTest'], threshold=0.12, seed=seed)\n",
    "train[train['isTest']==1].shape, train[train['isTest']==1]['air_store_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define evaluation metric\n",
    "def RMSE(y_true, y_pred):\n",
    "    return metrics.mean_squared_error(y_true, y_pred) ** 0.5\n",
    "rmse = metrics.make_scorer(metrics.mean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=23,\n",
       "           max_features=0.45, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=5, min_samples_split=21,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=-1,\n",
       "           oob_score=False, random_state=2018, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################################################################\n",
    "###                                           Training                                              ###\n",
    "#######################################################################################################\n",
    "reg = XGBRegressor(eta=0.01, learning_rate=0.01, n_estimators=10000, \n",
    "                   max_depth=8, min_child_weight=10,\n",
    "                   gamma=0.8,\n",
    "                   subsample=0.95, colsample_bytree=0.85,\n",
    "                   reg_alpha=2, reg_lambda=0,\n",
    "                   eval_metric='rmse', random_state=seed, seed=seed, n_jobs=-1, missing=-1,)\n",
    "reg\n",
    "\n",
    "'''\n",
    "reg = RandomForestRegressor(n_estimators=700, \n",
    "                            max_depth=23, \n",
    "                            min_samples_split=21, min_samples_leaf=5,\n",
    "                            max_features=0.45,\n",
    "                            random_state=seed, n_jobs=-1)\n",
    "reg\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ n_estimators: 800 #################\n",
      "RMSLE on training set: 0.375244769022\n",
      "RMSLE on validation set: 0.479432157866\n",
      "*************************************\n",
      "RMSLE on training set: 0.376162455569\n",
      "RMSLE on validation set: 0.475774051047\n",
      "*************************************\n",
      "RMSLE on training set: 0.374663182096\n",
      "RMSLE on validation set: 0.479531206343\n",
      "*************************************\n",
      "RMSLE on training set: 0.37455917417\n",
      "RMSLE on validation set: 0.480181070178\n",
      "*************************************\n",
      "RMSLE on training set: 0.374510755675\n",
      "RMSLE on validation set: 0.48255182364\n",
      "*************************************\n",
      "Average RMSLE on validation set: 0.479494061815 0.00217557255898\n"
     ]
    }
   ],
   "source": [
    "# Two-level cross-validation for selecting optimal parameters\n",
    "kf1 = model_selection.KFold(n_splits=5, shuffle=True, random_state=seed+2018030)\n",
    "kf2 = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed+2018030)\n",
    "\n",
    "for n_estimators in [800]:\n",
    "        print('################ n_estimators: {} #################'.format(n_estimators))\n",
    "        i = 0; rmsle_valid = np.array([0.] * 5)\n",
    "        for train_idx, valid_idx in kf1.split(train):\n",
    "            # first-level cross-validation\n",
    "            train_cv, valid_cv = train.iloc[train_idx], train.iloc[valid_idx]\n",
    "            X_train_cv, _, y_train_cv, _ = helper.getData(train_cv, valid_cv, seed=seed, ohe=True,\n",
    "                                                          agg_columns=agg, cat_columns=cat)\n",
    "            sample_weight = X_train_cv['weight']\n",
    "            \n",
    "            # second-level cross-validation, used for grand average of target encoding\n",
    "            valid_cv = helper.secondLevelCV(kf2, train_cv, valid_cv, agg_columns=agg, seed=seed)\n",
    "            X_valid_cv, y_valid_cv = helper.feature_encoder(valid_cv, cat)\n",
    "            X_valid_cv = X_valid_cv[X_train_cv.columns]\n",
    "            X_train_cv.drop(drop, axis=1, inplace=True)\n",
    "            X_valid_cv.drop(drop, axis=1, inplace=True)\n",
    "            \n",
    "            # impute missing data in validation set\n",
    "            neigh = KNeighborsRegressor(n_neighbors=5, weights='uniform', metric='euclidean', n_jobs=-1)\n",
    "            helper.imputer(X_train_cv, X_valid_cv, neigh)\n",
    "            \n",
    "            # assign sample weight to samples that have same distribution as test set\n",
    "            idx_trn, idx_val = X_train_cv['year'] == 1, X_valid_cv['year'] == 1\n",
    "            \n",
    "            reg.set_params(n_estimators=n_estimators)\n",
    "            reg.fit(X_train_cv.drop('air_store_id', axis=1), y_train_cv, sample_weight=sample_weight)\n",
    "            yhat_train = reg.predict(X_train_cv.drop('air_store_id', axis=1))[idx_trn]\n",
    "            yhat_valid = reg.predict(X_valid_cv.drop('air_store_id', axis=1))[idx_val]\n",
    "            print('RMSLE on training set:', RMSE(y_train_cv.loc[idx_trn], yhat_train))\n",
    "            print('RMSLE on validation set:', RMSE(y_valid_cv.loc[idx_val], yhat_valid))\n",
    "            print('*************************************')\n",
    "            rmsle_valid[i] = RMSE(y_valid_cv.loc[idx_val], yhat_valid); i += 1\n",
    "        print('Average RMSLE on validation set:', rmsle_valid.mean(), rmsle_valid.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE on entire training set: 0.37860233098\n"
     ]
    }
   ],
   "source": [
    "# Train on entire training set using optimal parameters and predict on test set\n",
    "'''kf2 = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed+20180106)\n",
    "\n",
    "X_train, _, y_train, _ = helper.getData(train, test, seed=seed, ohe=True,\n",
    "                                            agg_columns=agg, cat_columns=cat)\n",
    "sample_weight = X_train['weight']\n",
    "test = helper.secondLevelCV(kf2, train, test, agg_columns=agg, seed=seed)\n",
    "X_test, y_test = helper.feature_encoder(test, cat)\n",
    "X_test = X_test[X_train.columns]\n",
    "X_train.drop(drop, axis=1, inplace=True)\n",
    "X_test.drop(drop, axis=1, inplace=True)'''\n",
    "\n",
    "X_train, X_test, y_train, _ = helper.getData(train, test, seed=seed, ohe=True,\n",
    "                                            agg_columns=agg, cat_columns=cat)\n",
    "sample_weight = X_train['weight']\n",
    "X_train.drop(drop, axis=1, inplace=True)\n",
    "X_test.drop(drop, axis=1, inplace=True)\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors=5, weights='uniform', metric='euclidean', n_jobs=-1)\n",
    "helper.imputer(X_train, X_test, neigh)\n",
    "\n",
    "idx_trn = X_train['year'] == 1\n",
    "X_train['isTest'] = X_train['isTest'].astype(np.int8)\n",
    "X_test['isTest'] = X_test['isTest'].astype(np.int8)\n",
    "reg.fit(X_train.drop('air_store_id', axis=1), y_train, sample_weight=sample_weight)\n",
    "yhat_train = reg.predict(X_train.drop('air_store_id', axis=1))[idx_trn]\n",
    "yhat_test = reg.predict(X_test.drop('air_store_id', axis=1))\n",
    "print('RMSLE on entire training set:', RMSE(y_train.loc[idx_trn], yhat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate submission file\n",
    "sub = pd.concat([X_test.reset_index(drop=True), \n",
    "                 pd.DataFrame(yhat_test).rename(columns={0: 'visitors'})], axis=1).drop([\n",
    "    c for c in X_test.columns if c not in ['air_store_id', 'date_int', 'visitors']], axis=1)\n",
    "sub['date_int'] = sub['date_int'].astype(str).apply(lambda x:x[:4] + '-' + x[4:6] + '-' + x[6:])\n",
    "sub['id'] = sub['air_store_id'] + '_' + sub['date_int']\n",
    "sub.drop(['air_store_id', 'date_int'], axis=1, inplace=True)\n",
    "\n",
    "submission = pd.read_csv(path + '/input/sample_submission.csv').drop('visitors', axis=1)\n",
    "submission = submission.merge(sub, on='id', how='left')\n",
    "submission['visitors'] = np.expm1(submission['visitors']).clip(lower=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(path + '/output/train1_rf_20180125.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(path + '/output/train1_rf_20180125.data', 'wb') as f:\n",
    "    pickle.dump(reg, f)\n",
    "with open(path + '/output/columns_ordering.data', 'wb') as f:\n",
    "    pickle.dump(X_train_cv.columns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
